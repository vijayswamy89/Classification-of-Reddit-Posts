{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the information from reddit.com here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://www.reddit.com/r/nba.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2 = \"https://www.reddit.com/r/dogs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "res = requests.get(URL)\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = requests.get(URL_2)\n",
    "res2 = requests.get(URL_2, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = res2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "By default, Reddit will give you the top 25 posts. However, it is good to look at other features of the reddit data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "print(data['data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])\n"
     ]
    }
   ],
   "source": [
    "print(data2['data'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at both datasets, Reddit provides the same columns. However, it is important to go through all the reddit posts provided. This is where we need to look at the after column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = 't3_9aq4ns'\n",
    "new_url = 'http://www.reddit.com/r/nba.json?after='+after\n",
    "res = requests.get(URL, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = 't3_9aq4ns'\n",
    "new_url = 'http://www.reddit.com/r/dogs.json?after='+after\n",
    "res2 = requests.get(URL, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = res.json()\n",
    "new_data['data']['children'].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read through the sub-reddit nba and download a series of nba reddit posts. \n",
    "Reddit will limit the number of requests per second you're allowed to make and for this reason and for this reason, it is important to end the code with time.sleep(3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "url = 'http://www.reddit.com/r/nba.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data = res.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data['data']['children'][i]['data']['title'])\n",
    "        post.append(data['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nba.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read through the sub-reddit dogs and download a series of dogs reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/dogs.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res2 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data2 = res2.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data2['data']['children'][i]['data']['title'])\n",
    "        post.append(data2['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data2['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data2['data']['after']\n",
    "    url = 'http://www.reddit.com/r/dogs.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "Below, we will save the extracted information from reddit.com in a .csv file. Extracting information from reddit.com can be a rather time-consuming process and saving the results in a .csv file can allow for faster text mining and faster data analysis of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_posts, columns = ['Title','Text','Category'])\n",
    "df.to_csv(\"scraped-data.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will create two vectorizers: the count vectorizer and the term-frequency inverse document frequency vectorizer. What are these two things and why are they important? \n",
    "The count vectorizer simply counts the word frequencies. \n",
    "The term-frequency inverse document frequency vectorizer also counts word frequencies but it adjusts for words that occur more frequently. Such words include \"the\", \"and\", \"but\" that occur frequently but do not really add much value to the meaning of the post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='./scraped-data.csv', stop_words=sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "data = pd.read_csv('scraped-data.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='./scraped-data.csv', stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "data = pd.read_csv('scraped-data.csv',sep='\\t')\n",
    "data[\"Category\"] = data[\"Category\"].map(lambda x: 'dogs' in x)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorizer.fit_transform(data.Text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_output = tfidf_vectorizer.fit_transform(data.Text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in both output and tfidf_output need to be parsed into a format that can be readable by the train-test-split model, which is the next step in our process. For this reason, both are converted into arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert to a more readable format\n",
    "train_data_features = output.toarray()\n",
    "train_data_features_tfidf = tfidf_output.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 12183)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split is a very important part of predicting unseen data. Train-test split allows us to divide the sample data into two categories: train and test. The data in the train category is the data that our model is fed to make its predictions. The data in the test category is the data our model is exposed to and is expected to make predictions from using the data the model was fed in the train data. In our case, we will be making predictions of what posts belong in the sub-reddit nba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "x = train_data_features\n",
    "y = data[\"Category\"]\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(train_data_features, y, test_size=.3, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the logistic regression model to predict the categories of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "### Accuracy Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (True Positive + True Negative)/(Total Number of Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is the logistic regression model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "predicted = logistic.predict(x_train)\n",
    "train_score = logistic.score(x_train, y_train)\n",
    "score = logistic.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the logistic regression model holds value in correctly predicting which category the reddit post belongs in, it is still important to test other statistical models. One such model would be the Random Forest Classification Model. The Random Forest Classification Model is an ensemble model that creates multiple trees using different combinations of features of the reddit posts such as the title or the reddit text content and at the end chooses the best performing tree. We want to compare the accuracy score of a random forest classification model with the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285714285714286\n",
      "{'max_features': 'auto', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the accuracy scores of the logistic regression model and the random forest classifier, we can clearly see that the logistic regression model is better able to correctly predict the categories of the posts. In this case, the logistic regression model is better able to predict the posts that belong in the nba subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at two statistical models is great but we should try another model. This model is called a pipeline. Pipelining allows you to incorporate multiple models to help the model accurately predict the categories of reddit posts. In this case, I used a combination of selectkbest and randomforestclassifier to predict categories of posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95       107\n",
      "          1       0.97      0.93      0.95       103\n",
      "\n",
      "avg / total       0.95      0.95      0.95       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [    3     5     9 ... 12179 12181 12182] are constant.\n",
      "  UserWarning)\n",
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "select = sklearn.feature_selection.SelectKBest(k=100)\n",
    "clf = sklearn.ensemble.RandomForestClassifier()\n",
    "steps = [('feature_selection', select),\n",
    "        ('random_forest', clf)]\n",
    "parameters = {\"feature_selection__k\":[100, 200], \n",
    "              \"random_forest__n_estimators\":[50, 100, 200],\n",
    "              \"random_forest__min_samples_split\":[2, 3, 4, 5, 10]}\n",
    "pipeline = sklearn.pipeline.Pipeline(steps)\n",
    "\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "y_prediction = pipeline.predict(x_test)\n",
    "### test your predictions using sklearn.classification_report()\n",
    "report = sklearn.metrics.classification_report(y_test, y_prediction)\n",
    "### and print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, logistic regression continues to have the highest accuracy though the pipeline's accuracy of 95% is very strong as well. However, let's try two more models. I will choose the GradientBoostingClassifer and ExtraTreesClassifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier boosts the performance of a simple base-learner by iteratively shifting the focus towards problematic observations that are difficult to predict. That is, gradient boosting classifier identifies difficult observations by large residuals calculated from previous cycles. \n",
    "\n",
    "Extra Trees Classifier creates many trees of various segments of the data, and uses averaging to reduce error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9204081632653062\n",
      "{'max_features': 'auto', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(GradientBoostingClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9173469387755102\n",
      "{'max_features': 'sqrt', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2,7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(ExtraTreesClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make three paragraphs long. \n",
    "\n",
    "Reddit receives so many posts of a daily basis, and it is so crucial that Reddit successfully identifies the correct category of each post. Posts put in the correct category increases user engagement as it allows more users to share their knowledge with like-minded folks. While having volunteers or employees can also successfully categorize posts in their respective categories, it is a very difficult task to keep up with the large number of posts Reddit receives on a daily basis and also difficult to do so quickly. Luckily for you, my work will correctly put posts in the respective categories and ensure that users are always up to date on the latest information about the subreddit of their choice. \n",
    "\n",
    "In this data analysis report, I showcased how to extract NBA reddit posts and successfully identify future potential NBA reddit posts. I also gathered data from other subreddits to ensure that my predictions can differentiate basketball posts from other categories. As proof of how strong my predictions are, the data shows that my predictions can correctly identify 90+% of reddit posts with nba content as part of the nba subreddit. With such success, my work can take on other reddit posts and correctly identify those posts in their respective categories and in doing so, my work can save Reddit much time and money. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
