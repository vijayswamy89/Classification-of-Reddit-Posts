{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for the data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for five sub-reddits: nba, dogs, nfl, cars, and pizza. \n",
    "Our goal is to correctly identify reddit posts that belong in the nba but at the same time we want it to have more data to help our model differentiate posts that belong in the nba from other categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.reddit.com/r/nba.json\"\n",
    "url_2 = \"http://www.reddit.com/r/dogs.json\"\n",
    "url_3 = \"http://www.reddit.com/r/nfl.json\"\n",
    "url_4 = \"http://www.reddit.com/r/cars.json\"\n",
    "url_5 = \"http://www.reddit.com/r/pizza.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res = requests.get(url, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res2 = requests.get(url_2)\n",
    "res2 = requests.get(url_2, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res3 = requests.get(url_3)\n",
    "res3 = requests.get(url_3, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res4 = requests.get(url_4)\n",
    "res4 = requests.get(url_4, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res5 = requests.get(url_4)\n",
    "res5 = requests.get(url_4, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the reddit.com data into a more readable format to python. This is called Javascript Object Notation(JSON). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()\n",
    "data2 = res2.json()\n",
    "data3 = res3.json()\n",
    "data4 = res4.json()\n",
    "data5 = res5.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for nba posts so our model knows what nba subreddit posts consist of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "url = 'http://www.reddit.com/r/nba.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data = res.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data['data']['children'][i]['data']['title'])\n",
    "        post.append(data['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nba.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for posts in the sub-reddit dogs. This will be one of the categories not in nba so the model can better differentiate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/dogs.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res2 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data2 = res2.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data2['data']['children'][i]['data']['title'])\n",
    "        post.append(data2['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data2['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data2['data']['after']\n",
    "    url = 'http://www.reddit.com/r/dogs.json?after=' + after\n",
    "    \n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for posts found in the subreddit nfl. This would be to help differentiate nba posts from other sports-related posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/nfl.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res3 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data3 = res3.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data3['data']['children'][i]['data']['title'])\n",
    "        post.append(data3['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data3['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data3['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nfl.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for another category of unrelated data. This category is cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/cars.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(23):\n",
    "    res4 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data4 = res4.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data4['data']['children'][i]['data']['title'])\n",
    "        post.append(data4['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data4['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data4['data']['after']\n",
    "    url = 'http://www.reddit.com/r/cars.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for yet another sub-reddit different from the nba. I chose pizza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/pizza.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res5 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data5 = res5.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data5['data']['children'][i]['data']['title'])\n",
    "        post.append(data5['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data5['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data5['data']['after']\n",
    "    url = 'http://www.reddit.com/r/pizza.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write these reddit posts to a csv file for easier access later. Mining reddit.com can be a time-consuming process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_posts, columns = ['Title','Text','Category'])\n",
    "df.to_csv(\"scraped-data-multinomial.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will create the term-frequency inverse document frequency vectorizer. Why is this vectorizer important? The term-frequency inverse document frequency vectorizer also counts word frequencies but it adjusts for words that occur more frequently. Such words include \"the\", \"and\", \"but\" that occur frequently but do not really add much value to the meaning of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='./scraped-data.csv', stop_words=sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "data = pd.read_csv('scraped-data-multinomial.csv',sep='\\t')\n",
    "data[\"Category\"] = data[\"Category\"].map(lambda x: 'nba' in x)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorizer.fit_transform(data.Text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in both output and tfidf_output need to be parsed into a format that can be readable by the train-test-split model, which is the next step in our process. For this reason, both are converted into arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = output.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split is a very important part of predicting unseen data. Train-test split allows us to divide the sample data into two categories: train and test. The data in the train category is the data that our model is fed to make its predictions. The data in the test category is the data our model is exposed to and is expected to make predictions from using the data the model was fed in the train data. In our case, we will be making predictions of what posts belong in the sub-reddit nba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "x = train_data_features\n",
    "y = data[\"Category\"]\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(train_data_features, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the logistic regression model to predict the categories of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (True Positive + True Negative)/(Total Number of Values)\n",
    "\n",
    "How accurate is the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "predicted = logistic.predict(x_train)\n",
    "train_score = logistic.score(x_train, y_train)\n",
    "score = logistic.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the logistic regression model holds value in correctly predicting which category the reddit post belongs in, it is still important to test other statistical models. One such model would be the Random Forest Classification Model. The Random Forest Classification Model is an ensemble model that creates multiple trees using different combinations of features of the reddit posts such as the title or the reddit text content and at the end chooses the best performing tree. We want to compare the accuracy score of a random forest classification model with the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9184898354307841\n",
      "{'max_features': 'auto', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the accuracy scores of the logistic regression model and the random forest classifier, we can clearly see that the Random Forest model is better able to correctly predict the categories of the posts. In this case, the Random Forest model is better able to predict the posts that belong in the nba subreddit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
