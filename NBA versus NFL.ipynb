{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the information from reddit.com here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.reddit.com/r/nba.json\"\n",
    "url_2 = \"http://www.reddit.com/r/nfl.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res = requests.get(url, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res2 = requests.get(url_2)\n",
    "res2 = requests.get(url_2, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()\n",
    "data2 = res2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read through the sub-reddit nba and download a series of nba reddit posts. \n",
    "Reddit will limit the number of requests per second you're allowed to make and for this reason and for this reason, it is important to end the code with time.sleep(3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "url = 'http://www.reddit.com/r/nba.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data = res.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data['data']['children'][i]['data']['title'])\n",
    "        post.append(data['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nba.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read through the sub-reddit NFL and download a series of NFL reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/nfl.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res2 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data2 = res2.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data2['data']['children'][i]['data']['title'])\n",
    "        post.append(data2['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data2['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data2['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nfl.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_posts, columns = ['Title','Text','Category'])\n",
    "df.to_csv(\"nfl-nba.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='./scraped-data.csv', stop_words=sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "data = pd.read_csv('scraped-data-multinomial.csv',sep='\\t')\n",
    "data[\"Category\"] = data[\"Category\"].map(lambda x: 'nba' in x)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorizer.fit_transform(data.Text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = output.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split is a very important part of predicting unseen data. Train-test split allows us to divide the sample data into two categories: train and test. The data in the train category is the data that our model is fed to make its predictions. The data in the test category is the data our model is exposed to and is expected to make predictions from using the data the model was fed in the train data. In our case, we will be making predictions of what posts belong in the sub-reddit nba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "x = train_data_features\n",
    "y = data[\"Category\"]\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(train_data_features, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (True Positive + True Negative)/(Total Number of Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "predicted = logistic.predict(x_train)\n",
    "train_score = logistic.score(x_train, y_train)\n",
    "score = logistic.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the logistic regression model holds value in correctly predicting which category the reddit post belongs in, it is still important to test other statistical models. One such model would be the Random Forest Classification Model. The Random Forest Classification Model is an ensemble model that creates multiple trees using different combinations of features of the reddit posts such as the title or the reddit text content and at the end chooses the best performing tree. We want to compare the accuracy score of a random forest classification model with the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9173281703775411\n",
      "{'max_features': 'sqrt', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the accuracy scores of the logistic regression model and the random forest classifier, we can clearly see that the Random Forest model is better able to correctly predict the categories of the posts. In this case, the Random Forest model is better able to predict the posts that belong in the nba subreddit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
