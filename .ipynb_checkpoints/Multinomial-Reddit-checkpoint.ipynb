{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for the data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for five sub-reddits: nba, dogs, nfl, cars, and pizza. \n",
    "Our goal is to correctly identify reddit posts that belong in the nba but at the same time we want it to have more data to help our model differentiate posts that belong in the nba from other categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.reddit.com/r/nba.json\"\n",
    "url_2 = \"http://www.reddit.com/r/dogs.json\"\n",
    "url_3 = \"http://www.reddit.com/r/nfl.json\"\n",
    "url_4 = \"http://www.reddit.com/r/cars.json\"\n",
    "url_5 = \"http://www.reddit.com/r/pizza.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res = requests.get(url, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res2 = requests.get(url_2)\n",
    "res2 = requests.get(url_2, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res3 = requests.get(url_3)\n",
    "res3 = requests.get(url_3, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res4 = requests.get(url_4)\n",
    "res4 = requests.get(url_4, headers={'User-agent': 'Vijay Bot 0.1'})\n",
    "res5 = requests.get(url_4)\n",
    "res5 = requests.get(url_4, headers={'User-agent': 'Vijay Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the reddit.com data into a more readable format to python. This is called Javascript Object Notation(JSON). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()\n",
    "data2 = res2.json()\n",
    "data3 = res3.json()\n",
    "data4 = res4.json()\n",
    "data5 = res5.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for nba posts so our model knows what nba subreddit posts consist of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = []\n",
    "url = 'http://www.reddit.com/r/nba.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data = res.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data['data']['children'][i]['data']['title'])\n",
    "        post.append(data['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nba.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for posts in the sub-reddit dogs. This will be one of the categories not in nba so the model can better differentiate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/dogs.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res2 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data2 = res2.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data2['data']['children'][i]['data']['title'])\n",
    "        post.append(data2['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data2['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data2['data']['after']\n",
    "    url = 'http://www.reddit.com/r/dogs.json?after=' + after\n",
    "    \n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for posts found in the subreddit nfl. This would be to help differentiate nba posts from other sports-related posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/nfl.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res3 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data3 = res3.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data3['data']['children'][i]['data']['title'])\n",
    "        post.append(data3['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data3['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data3['data']['after']\n",
    "    url = 'http://www.reddit.com/r/nfl.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for another category of unrelated data. This category is cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/cars.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(23):\n",
    "    res4 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data4 = res4.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data4['data']['children'][i]['data']['title'])\n",
    "        post.append(data4['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data4['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data4['data']['after']\n",
    "    url = 'http://www.reddit.com/r/cars.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine reddit.com for yet another sub-reddit different from the nba. I chose pizza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.reddit.com/r/pizza.json?after='\n",
    "#each of these requests below does 26 and then 25 posts\n",
    "for i in range(28):\n",
    "    res5 = requests.get(url, headers={'User-agent': 'vijay'})\n",
    "    data5 = res5.json()\n",
    "    for i in range(25):\n",
    "        post = []\n",
    "        post.append(data5['data']['children'][i]['data']['title'])\n",
    "        post.append(data5['data']['children'][i]['data']['selftext'])\n",
    "        post.append(data5['data']['children'][i]['data']['subreddit'])\n",
    "#     post.append(data['data']['children']['title'])\n",
    "#     post.append(data['data']['children']['subreddit_name_prefixed'])\n",
    "#     all_posts.append(post)\n",
    "        all_posts.append(post)\n",
    "    after = data5['data']['after']\n",
    "    url = 'http://www.reddit.com/r/pizza.json?after=' + after\n",
    "    \n",
    "    #print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write these reddit posts to a csv file for easier access later. Mining reddit.com can be a time-consuming process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_posts, columns = ['Title','Text','Category'])\n",
    "df.to_csv(\"scraped-data-multinomial.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will create the term-frequency inverse document frequency vectorizer. Why is this vectorizer important? The term-frequency inverse document frequency vectorizer also counts word frequencies but it adjusts for words that occur more frequently. Such words include \"the\", \"and\", \"but\" that occur frequently but do not really add much value to the meaning of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='./scraped-data.csv', stop_words=sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "data = pd.read_csv('scraped-data-multinomial.csv',sep='\\t')\n",
    "data[\"Category\"] = data[\"Category\"].map(lambda x: 'nba' in x)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2956\n",
       "False    2315\n",
       "Name: Text, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Text\"].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the null values from the text by replacing those with the titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data[\"Text\"].isna:\n",
    "    data[\"Text\"] = data[\"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorizer.fit_transform(data.Text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in both output and tfidf_output need to be parsed into a format that can be readable by the train-test-split model, which is the next step in our process. For this reason, both are converted into arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = output.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split is a very important part of predicting unseen data. Train-test split allows us to divide the sample data into two categories: train and test. The data in the train category is the data that our model is fed to make its predictions. The data in the test category is the data our model is exposed to and is expected to make predictions from using the data the model was fed in the train data. In our case, we will be making predictions of what posts belong in the sub-reddit nba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "x = train_data_features\n",
    "y = data[\"Category\"]\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(train_data_features, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the breakdown of reddit posts categorized as nba posts and reddit posts not categorized as nba posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/dsi/lib/python3.6/site-packages/seaborn/categorical.py:1428: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
      "  stat_data = remove_na(group_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a15ec4d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data[\"Category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the logistic regression model to predict the categories of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (True Positive + True Negative)/(Total Number of Values)\n",
    "\n",
    "How accurate is the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9056603773584906\n"
     ]
    }
   ],
   "source": [
    "predicted = logistic.predict(x_train)\n",
    "train_score = logistic.score(x_train, y_train)\n",
    "score = logistic.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the logistic regression model holds value in correctly predicting which category the reddit post belongs in, it is still important to test other statistical models. One such model would be the Random Forest Classification Model. The Random Forest Classification Model is an ensemble model that creates multiple trees using different combinations of features of the reddit posts such as the title or the reddit text content and at the end chooses the best performing tree. We want to compare the accuracy score of a random forest classification model with the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9366892545982575\n",
      "{'max_features': 'sqrt', 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "param_grid = { \n",
    "    'n_estimators': [2, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(), param_grid = param_grid)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      4480\n",
      "          1       1.00      0.93      0.96       685\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the accuracy scores of the logistic regression model and the random forest classifier, we can clearly see that the Random Forest model is better able to correctly predict the categories of the posts. In this case, the Random Forest model is better able to predict the posts that belong in the nba subreddit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is wonderful to have great accuracy scores, we want to find the top 50 words most commonly associated with the nba subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nba</th>\n",
       "      <td>3.926095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lebron</th>\n",
       "      <td>2.471383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketball</th>\n",
       "      <td>1.870832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <td>1.771122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deng</th>\n",
       "      <td>1.695037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goat</th>\n",
       "      <td>1.635428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finals</th>\n",
       "      <td>1.623049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wnba</th>\n",
       "      <td>1.608673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>players</th>\n",
       "      <td>1.595735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lakers</th>\n",
       "      <td>1.571739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "nba         3.926095\n",
       "lebron      2.471383\n",
       "basketball  1.870832\n",
       "kd          1.771122\n",
       "deng        1.695037\n",
       "goat        1.635428\n",
       "finals      1.623049\n",
       "wnba        1.608673\n",
       "players     1.595735\n",
       "lakers      1.571739"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=vectorizer.get_feature_names()\n",
    "coef = pd.DataFrame(logistic.coef_, columns=columns)\n",
    "df_coef = coef.T.sort_values(by = 0, ascending=False)\n",
    "df_coef.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a one unit increase in the word \"nba\", the probability of a reddit post being an nba subreddit increases by 3.92 times. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
